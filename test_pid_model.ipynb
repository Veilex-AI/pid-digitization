{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The test_pid_model ipnb file calculates the accuracy of the model bounding box and label prediction using Mean square error and percentage based accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "class_area = {'21': 1985, '2': 5415, '18': 6969, '26': 17849, '8': 7037, '20': 6110, '25': 6532, '32': 16411, '28': 17179, '7': 6771, '9': 6689, '12': 5510, '22': 5795, '24': 2808, '11': 6129, '23': 8645, '29': 17181, '4': 6132, '31': 16001, '5': 4808, '15': 3737, '17': 5721, '19': 6769, '30': 11940, '16': 3564, '13': 6558, '1': 4575, '6': 3126, '10': 8179, '27': 18761, '14': 13607, '3': 6903}\n",
    "\n",
    "TEST_DATASET_PATH = \"D:/Veilex/_test/train-segment-data/datasets/custom_dataset/dataset/test\"\n",
    "FULL_TEST_DATASET_PATH=\"D:/Veilex/_test/dataset/DigitizePID_Dataset\"\n",
    "images_dir = \"images\"\n",
    "labels_dir = \"labels\"\n",
    "\n",
    "# the file id for the model in google dirve\n",
    "MODEL_FILE_ID = \"1rd0GmerO8wzWBcAr5e38_WOXYLJqTDKl\"\n",
    "model_file_id = MODEL_FILE_ID\n",
    "model_file_name = \"yolo-model-pid.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "CREDENTIAL_FILE_NAME = \"./credentials.json\"\n",
    "TOKEN_FILE_NAME = \"./token.json\"\n",
    "\n",
    "def download_file(file_id, credential_file_name: str):\n",
    "    file = None\n",
    "    creds = None\n",
    "\n",
    "    if os.path.exists(TOKEN_FILE_NAME):\n",
    "        creds = Credentials.from_authorized_user_file(TOKEN_FILE_NAME, SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                credential_file_name, SCOPES\n",
    "            )\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open(TOKEN_FILE_NAME, \"w\") as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    try:\n",
    "        service = build(\"drive\", \"v3\", credentials=creds)\n",
    "\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        file = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(file, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Download {int(status.progress() * 100)}.\")\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "        file = None\n",
    "\n",
    "    return file.getvalue()\n",
    "\n",
    "def upload_yolo_file(model_file_id: str):\n",
    "    yolo_model_pid_bytes = download_file(file_id=model_file_id, credential_file_name=CREDENTIAL_FILE_NAME)\n",
    "    if not os.path.exists(f\"./{model_file_name}\"):\n",
    "        with open(f\"./{model_file_name}\", 'wb') as f:\n",
    "            f.write(yolo_model_pid_bytes)\n",
    "\n",
    "    return f\"./{model_file_name}\"\n",
    "\n",
    "# file_path = upload_yolo_file(model_file_id)\n",
    "file_path = download_file_with_service_account(MODEL_FILE_ID, \"./service-account-key.json\")\n",
    "model_file_path = file_path if file_path is not None else f\"./{model_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "model = YOLO(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "from src.models import Vertex, PidDataPoint, Symbol\n",
    "from src.utils import denormalize_coordinates\n",
    "\n",
    "# this is a YOLO formatted based dataset that has to be extracted from the directory.\n",
    "def read_yolo_label_data(label_path: str) -> List[Tuple[str, float, float, float, float]]:\n",
    "    with open(label_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        class_id, x, y, _x, _y = line.strip().split()\n",
    "        labels.append((class_id, float(x), float(y), float(_x), float(_y)))\n",
    "    return labels\n",
    "    \n",
    "\n",
    "pid_test_dataset = []\n",
    "\n",
    "dataset_images_path = f\"{TEST_DATASET_PATH}/{images_dir}\"\n",
    "dataset_labels_path = f\"{TEST_DATASET_PATH}/{labels_dir}\"\n",
    "image_files = [os.path.join(dataset_images_path, f) for f in os.listdir(dataset_images_path) if os.path.isfile(os.path.join(dataset_images_path, f))]\n",
    "\n",
    "for image_file_path in image_files:\n",
    "    datapoint = PidDataPoint(\n",
    "        image_path=image_file_path\n",
    "    )\n",
    "    image = Image.open(image_file_path)\n",
    "    image_file_name = '.'.join(image_file_path.split(\"\\\\\")[-1].split(\".\")[0:-1])\n",
    "    label_dp_path = os.path.join(dataset_labels_path, f\"{image_file_name}.txt\")\n",
    "    if os.path.isfile(label_dp_path):\n",
    "        yolo_datalabel = read_yolo_label_data(label_dp_path)\n",
    "        symbols = []\n",
    "        for l in yolo_datalabel:\n",
    "            class_id, x, y, _x, _y = l\n",
    "            x, y, _x, _y = denormalize_coordinates([x, y, _x, _y], image.width, image.height)\n",
    "            symbols.append(\n",
    "                Symbol(\n",
    "                    label=class_id,\n",
    "                    name=\"\",\n",
    "                    pointSrc=Vertex(x=x, y=y),\n",
    "                    pointDest=Vertex(x=_x, y=_y)\n",
    "                )\n",
    "            )\n",
    "        datapoint.symbols = symbols\n",
    "    \n",
    "    pid_test_dataset.append(datapoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services import PredictSymbolsService\n",
    "\n",
    "\n",
    "def predict_results():\n",
    "    predicted_results = []\n",
    "    for index, d in enumerate(pid_test_dataset[:]):\n",
    "        predict_service = PredictSymbolsService(\n",
    "            image_path=d.image_path,\n",
    "            model_path=model_file_path\n",
    "        )\n",
    "\n",
    "        bboxes_with_labels = predict_service.predict_bounding_boxes()\n",
    "        predicted_results.append(bboxes_with_labels)\n",
    "\n",
    "    return predicted_results\n",
    "\n",
    "predicted_results: List[List[Tuple[List[int], str]]] = predict_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the symbols that have been detected but were cutted of by segmenting the images.\n",
    "def filter_out_cutoff_symbols(symbols: list[list[Tuple[list[int, int, int, int], str]]]):\n",
    "    symbols_filtered = []\n",
    "    for symbol in symbols:\n",
    "        filtered = []\n",
    "        for coordinates in symbol:\n",
    "            x, y, _x, _y = coordinates[0]\n",
    "            label = coordinates[1]\n",
    "            area = abs(_x - x) * abs(_y - y)\n",
    "            if(area >= class_area[label] * 0.7):\n",
    "                filtered.append(coordinates)\n",
    "        symbols_filtered.append(filtered)\n",
    "\n",
    "    return symbols_filtered\n",
    "\n",
    "predicted_results = filter_out_cutoff_symbols(\n",
    "    predicted_results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.bounding_box import BoundingBox\n",
    "from src.services.image_display_serivce import ImageDisplayService\n",
    "from src.utils import convert_raw_data_to_bounding_box\n",
    "\n",
    "def dispaly_pid_image(index: int):\n",
    "    display_service = ImageDisplayService(\n",
    "        image_path=pid_test_dataset[index].image_path,\n",
    "        bounding_boxes= [\n",
    "            BoundingBox(\n",
    "                name=n[1],\n",
    "                pointSrc=Vertex(x=int(n[0][0]), y=int(n[0][1])),\n",
    "                pointDest=Vertex(x=int(n[0][2]), y=int(n[0][3]))\n",
    "            )\n",
    "            for n in predicted_results[index]\n",
    "        ]\n",
    "    )\n",
    "    display_service.display_image_with_bbox()\n",
    "\n",
    "dispaly_pid_image(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# mean square error calculation w.r.t the bounding boxes.\n",
    "\n",
    "def mean_square_error_with_matching(actual_boxes, predicted_boxes) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mean square error (MSE) between actual and predicted bounding boxes,\n",
    "    accounting for mismatched order by using optimal matching (Hungarian algorithm).\n",
    "    \n",
    "    Parameters:\n",
    "    actual_boxes: List of actual bounding boxes [(x1, y1, x2, y2), ...]\n",
    "    predicted_boxes: List of predicted bounding boxes [(x1, y1, x2, y2), ...]\n",
    "\n",
    "    Returns:\n",
    "    float: Mean square error value\n",
    "    \"\"\"\n",
    "\n",
    "    if len(actual_boxes) > len(predicted_boxes):\n",
    "        for _ in range(abs(len(actual_boxes) - len(predicted_boxes))):\n",
    "            predicted_boxes.append([0, 0, 0, 0])\n",
    "    \n",
    "    if len(predicted_boxes) > len(actual_boxes):\n",
    "        for _ in range(abs(len(actual_boxes) - len(predicted_boxes))):\n",
    "            actual_boxes.append([0, 0, 0, 0])\n",
    "\n",
    "    if len(actual_boxes) != len(predicted_boxes):\n",
    "        raise ValueError(\"The number of actual and predicted boxes must be the same.\")\n",
    "    \n",
    "    actual_boxes = np.array(actual_boxes)\n",
    "    predicted_boxes = np.array(predicted_boxes)\n",
    "    \n",
    "    # Compute the cost matrix (pairwise squared errors)\n",
    "    cost_matrix = np.zeros((len(actual_boxes), len(predicted_boxes)))\n",
    "    for i, actual in enumerate(actual_boxes):\n",
    "        for j, predicted in enumerate(predicted_boxes):\n",
    "            cost_matrix[i, j] = np.mean((actual - predicted) ** 2)\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    mse = 0.0\n",
    "    for i, j in zip(row_ind, col_ind):\n",
    "        mse += cost_matrix[i, j]\n",
    "    \n",
    "    mse /= len(actual_boxes)\n",
    "    return mse\n",
    "\n",
    "def bbox_area_valid(annotation_prediction):\n",
    "    bbox = annotation_prediction[0]\n",
    "    class_id = annotation_prediction[1]\n",
    "    predicted_area = (abs(bbox[0] - bbox[2])) * (abs(bbox[1] - bbox[3]))\n",
    "    return predicted_area > class_area[class_id] * 0.6\n",
    "\n",
    "# mean squre error in testing the images with symbols\n",
    "def calculate_test_pid_mse():\n",
    "    no_of_test_data = len(predicted_results)\n",
    "    not_including_in_testing = 0\n",
    "    sum = 0\n",
    "    for index, d in enumerate(predicted_results):\n",
    "        # filtered_result = [bl for bl in d if bbox_area_valid(bl)] # [bl[0] for bl in d]\n",
    "        predicted_bboxes = [bl[0] for bl in d] # [bl[0] for bl in filtered_result] \n",
    "        actual_bboxes = []\n",
    "        for s in pid_test_dataset[index].symbols:\n",
    "            x, y = s.pointSrc.get_dimensions()\n",
    "            _x, _y = s.pointDest.get_dimensions()\n",
    "            actual_bboxes.append([x, y, _x, _y])\n",
    "        \n",
    "        mse = 0\n",
    "        try:\n",
    "            mse += mean_square_error_with_matching(actual_bboxes, predicted_bboxes)\n",
    "        except:\n",
    "            not_including_in_testing += 1\n",
    "            print(f\"could not caculate MSE of the datapoint {index}\") \n",
    "\n",
    "        sum += mse\n",
    "    print(\"not included in dataset \", not_including_in_testing)\n",
    "    print(\"Mean Square Error, \", sum / (no_of_test_data - not_including_in_testing))\n",
    "\n",
    "calculate_test_pid_mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "# accuracy of dataset w.r.t the bounding boxes.\n",
    "\n",
    "def calculate_iou(box1, box2) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    box1, box2: (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def accuracy_with_iou(actual_boxes, predicted_boxes, iou_threshold=0.5) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage accuracy of the predicted bounding boxes using IoU.\n",
    "    \n",
    "    Parameters:\n",
    "    actual_boxes: List of actual bounding boxes [(x1, y1, x2, y2), ...]\n",
    "    predicted_boxes: List of predicted bounding boxes [(x1, y1, x2, y2), ...]\n",
    "    iou_threshold: IoU threshold to consider a match as correct\n",
    "    \n",
    "    Returns:\n",
    "    float: Accuracy percentage\n",
    "    \"\"\"\n",
    "    if len(actual_boxes) > len(predicted_boxes):\n",
    "        for _ in range(abs(len(actual_boxes) - len(predicted_boxes))):\n",
    "            predicted_boxes.append([0, 0, 0, 0])\n",
    "    \n",
    "    if len(predicted_boxes) > len(actual_boxes):\n",
    "        for _ in range(abs(len(actual_boxes) - len(predicted_boxes))):\n",
    "            actual_boxes.append([0, 0, 0, 0])\n",
    "\n",
    "    if len(actual_boxes) != len(predicted_boxes):\n",
    "        raise ValueError(\"The number of actual and predicted boxes must be the same.\")\n",
    "    \n",
    "    # Compute the IoU cost matrix\n",
    "    cost_matrix = np.zeros((len(actual_boxes), len(predicted_boxes)))\n",
    "    for i, actual in enumerate(actual_boxes):\n",
    "        for j, predicted in enumerate(predicted_boxes):\n",
    "            cost_matrix[i, j] = -calculate_iou(actual, predicted)  # Negate for maximization\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    correct_matches = 0\n",
    "    for i, j in zip(row_ind, col_ind):\n",
    "        iou = -cost_matrix[i, j]  # Undo the negation\n",
    "        if iou >= iou_threshold:\n",
    "            correct_matches += 1\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = (correct_matches / len(actual_boxes)) * 100\n",
    "    return accuracy\n",
    "\n",
    "# percentage accuracy of PID\n",
    "def calculate_test_pid_accuracy():\n",
    "    no_of_test_dataset = len(predicted_results)\n",
    "    not_including_in_testing = 0\n",
    "    sum = 0\n",
    "    for index, d in enumerate(predicted_results):\n",
    "        predicted_bboxes = [bl[0] for bl in d]\n",
    "\n",
    "        actual_bboxes = []\n",
    "        for s in pid_test_dataset[index].symbols:\n",
    "            x, y = s.pointSrc.get_dimensions()\n",
    "            _x, _y = s.pointDest.get_dimensions()\n",
    "            actual_bboxes.append([x, y, _x, _y])\n",
    "        \n",
    "        accuracy = 0\n",
    "        try:\n",
    "            accuracy += accuracy_with_iou(actual_bboxes, predicted_bboxes)\n",
    "        except:\n",
    "            not_including_in_testing += 1\n",
    "            print(f\"Could not test MSE of the datapoint {index}\") \n",
    "\n",
    "        sum += accuracy\n",
    "\n",
    "    print(\"Accuracy\")\n",
    "    print(sum / (no_of_test_dataset - not_including_in_testing))\n",
    "    print(\"datapoints not included in testing \", not_including_in_testing)\n",
    "\n",
    "calculate_test_pid_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of label accuracy with the segmented dataset.\n",
    "\n",
    "def calculate_label_accuracy(predicted_result_set, actual_dataset):\n",
    "    sum_accuracy = 0\n",
    "    pid_test_dataset = actual_dataset\n",
    "    for index, d in enumerate(predicted_result_set):\n",
    "        predicted_labels = [x[1] for x in d]\n",
    "        actual_symbol_labels = [s.label for s in pid_test_dataset[index].symbols]\n",
    "        s_dict = {}\n",
    "        for s in actual_symbol_labels:\n",
    "            if(s not in s_dict):\n",
    "                s_dict[s] = 1\n",
    "            else:\n",
    "                s_dict[s] += 1\n",
    "\n",
    "        for s in predicted_labels:\n",
    "            if(s in s_dict):\n",
    "                s_dict[s] -= 1\n",
    "\n",
    "        incorrect_result_sum = sum([abs(val) for val in s_dict.values()])\n",
    "\n",
    "        sum_accuracy += ((len(actual_symbol_labels) - incorrect_result_sum) / len(actual_symbol_labels))\n",
    "\n",
    "    return sum_accuracy / len(predicted_result_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the accuracy on full images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_dataset_path = \"D:/Veilex/_test/dataset/DigitizePID_Dataset\"\n",
    "from config import config\n",
    "from src.services import DataConverterService, PredictSymbolsService\n",
    "\n",
    "# change the config here. This should not be done anywhere else btw, this is exception\n",
    "config.dataset_path = pid_dataset_path\n",
    "config.image_dir_name = \"image_2\"\n",
    "config.annotation_dir_name = \"annotations\"\n",
    "\n",
    "data_service = DataConverterService()\n",
    "full_pid_dataset = data_service.load_dataset()\n",
    "\n",
    "def predict_full_dataset_results():\n",
    "    predicted_results = []\n",
    "    for d in full_pid_dataset[0:20]:\n",
    "        predict_service = PredictSymbolsService(\n",
    "            image_path=d.image_path,\n",
    "            model_path=model_file_path\n",
    "        )\n",
    "\n",
    "        bboxes_with_labels = predict_service.predict_bounding_boxes(shifting=True)\n",
    "        predicted_results.append(bboxes_with_labels)\n",
    "\n",
    "    return predicted_results\n",
    "\n",
    "predicted_full_results: List[PidDataPoint] = predict_full_dataset_results()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import convert_points_to_bounding_box\n",
    "from src.services import ImageDisplayService\n",
    "\n",
    "index_to_select = 9\n",
    "\n",
    "instance = predicted_full_results[index_to_select]\n",
    "service = ImageDisplayService(\n",
    "    image_path=full_pid_dataset[index_to_select].image_path,\n",
    "    bounding_boxes=[convert_points_to_bounding_box(i[0]) for i in instance ]\n",
    ")\n",
    "\n",
    "service.display_image_with_bbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_full_pid_dataset_accuracy():\n",
    "    not_including_in_testing = 0\n",
    "    sum = 0\n",
    "    for index, d in enumerate(predicted_full_results):\n",
    "        predicted_bboxes = [bl[0] for bl in d]\n",
    "\n",
    "        actual_bboxes = []\n",
    "        for s in full_pid_dataset[index].symbols:\n",
    "            x, y = s.pointSrc.get_dimensions()\n",
    "            _x, _y = s.pointDest.get_dimensions()\n",
    "            actual_bboxes.append([x, y, _x, _y])\n",
    "        \n",
    "        accuracy = 0\n",
    "        try:\n",
    "            print( len(actual_bboxes), len(predicted_bboxes) )\n",
    "            accuracy += accuracy_with_iou(actual_bboxes, predicted_bboxes)\n",
    "        except:\n",
    "            not_including_in_testing += 1\n",
    "            print(f\"Could not test MSE of the datapoint {index}\") \n",
    "\n",
    "        sum += accuracy\n",
    "\n",
    "    print(\"Accuracy\")\n",
    "    print(sum / (len(predicted_full_results) - not_including_in_testing))\n",
    "    print(\"datapoints not included in testing \", not_including_in_testing)\n",
    "\n",
    "calculate_full_pid_dataset_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_label_accuracy(predicted_full_results, full_pid_dataset[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
